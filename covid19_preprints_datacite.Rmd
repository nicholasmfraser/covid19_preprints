---
title: "R Notebook"
output: github_document
---

# Load required packages 

```{r message=FALSE, warning=FALSE}

library(tidyverse)
library(lubridate)
library(rdatacite)
library(ratelimitr)
library(unikn) # colorblind friendly palette

```

# Datacite data

The first step is to harvest raw metadata of preprints indexed in Datacite. For harvesting of Datacite metadata the [rdatacite](https://github.com/ropensci/rcrossref) package for R was used. In general, preprints are indexed in Datacite with the 'resourceType' field set to 'Preprint'. This field is not strictly controlled though, so not all preprints will be caught this way. 

Results can be filtered on date of creation (year only), which works fine for 2020 (but will need adaptation afterwards). Pagination is used to get all records.

```{r cache= TRUE}

#define function to query Datacite API
getDataCite <- function(n){
  res <- dc_dois(query = "types.resourceType:Preprint", 
                created = "2020",
                limit = 1000,
                page = n)
}  


#define function to add progress bar
getDataCite_progress <- function(n){
  pb$tick()$print()
  res <- getDataCite(n)
  
  return(res)
}

#initial query to get number of results for pagination
dois <- dc_dois(query = "types.resourceType:Preprint", 
                created = "2020",
                limit = 1)
total <- dois$meta$total

#create pagination sequence
seq <- seq(1, total, 1000)
seq <- c(1:length(seq))

#set counter for progress bar
pb <- progress_estimated(length(seq))

#get datacite results
data <- map(seq, getDataCite_progress)

rm(pb)


```


Next, relevant preprint metadata fields are parsed from the list format returned in the previous step, to a more manageable data frame. Note that specific preprint repositories are encoded in the field 'client', and abstracts are included in the field 'descriptions'. The resulting columns 'title' and 'descriptions' are list columns and need to be processed further to extract the needed information. 

```{r, cache= TRUE}

parsePreprints <- function(item) {
  res <- tibble(
    doi = item$data$attributes$doi,
    created = as.Date(item$data$attributes$created),
    client = item$data$relationships$client$data$id,
    title = item$data$attributes$titles,
    descriptions = item$data$attributes$descriptions
  )
}

data_df <- map_dfr(data, parsePreprints)

```

The resulting columns 'title' and 'descriptions' are list columns and need to be processed further to extract the needed information. 

```{r cache= TRUE}

parseTitles <- function(x){
  x <- x %>% 
    #add column with unique ID to enable join later on
    mutate(IDseq = rownames(.)) %>%
    #unnest list column, remove non-needed variables
    #unnest also creates new rows for records with multiple titles
    unnest(title) %>%
    select(-c(titleType,lang, descriptions)) %>%
    #some records have multiple titles (e.g. title and subtitle), 
    #collapse these into 1 character string
    group_by(IDseq, doi, created, client) %>%
    summarize(title = str_c(title, collapse = "; ")) %>%
    ungroup()
  } 
  
parseDescriptions <- function(x){
  x <- x %>%
    #add column with unique ID to enable join later on
    mutate(IDseq = rownames(.)) %>%
    select(IDseq, everything()) %>%
    #unnest list column description
    #only keep records with descriptionType 'Abstract'
    unnest(descriptions, keep_empty = TRUE) %>%
    filter(descriptionType == "Abstract") %>%
    rename(abstract = description) %>%
    select(-c(descriptionType,lang, title)) %>%
    #some records have multiple abstracts (often with other information...) 
    #collapse these into 1 character string
    group_by(IDseq, doi, created, client) %>%
    summarize(abstract = str_c(abstract, collapse = "; ")) %>%
    ungroup()
  }

 
data_df_title <- parseTitles(data_df)
data_df_abstract <- parseDescriptions(data_df)

data_df <- data_df_title %>%
  left_join(data_df_abstract, by = c("IDseq", "doi", "created", "client")) %>%
  select(-IDseq)

rm(data_df_title, data_df_abstract)

```


In the final step, preprints are subsetted to include only those related to COVID-19, and the respective preprint repository of each identified.

```{r, cache = TRUE}

# Generate a search string containing terms related to COVID-19
search_string <- "coronavirus|covid|sars-cov|ncov-2019|2019-ncov"

data_df_covid <- data_df %>%
  #rename date column to allow reuse of code used for Crossref data
  rename(posted_date = created) %>%
  # Filter COVID-19 related preprints
  filter(str_detect(title, regex(search_string, ignore_case = TRUE)) | 
         str_detect(abstract, regex(search_string, ignore_case = TRUE))) %>%
  # Rule-based matching of preprints to repositories.
  # Repository names are encoded in field 'client.
  mutate(source = case_when(
    client == "rg.rg" ~ "ResearchGate",
    client == "figshare.ars" ~ "Figshare",
    client == "cern.zenodo" ~ "Zenodo",
    TRUE ~ NA_character_)) %>%
  # Remove those that could not be unambiguously matched
  filter(!is.na(source)) %>%
  # Some preprints have multiple DOI records relating to multiple preprint
  # versions. It differs how this is reflected in the DOIs. 
  # In Figshare and ResearchGate, the DOI is appended with a version number
  #(e.g.10.1000/12345.v2 and 10.1000/12345/1, respectively)
  # To ensure only a single record is counted per preprint, the version number is
  # removed and only the earliest DOI record is kept
  #mutate(doi_clean = str_replace(doi, "\\.v.*|\\/v.*", "")) %>%
  mutate(doi_clean = str_replace(doi, "\\.v.*|\\/[0-9]$|\\/[1-9][0-9]$", "")) %>%
  group_by(doi_clean) %>%
  arrange(posted_date) %>%
  slice(1) %>%
  ungroup() %>%
  # Additionally filter preprints with the same title posted on the same server
  # This will also address versionin on Zenodo, where consecutive DOIs are used
  group_by(source, title) %>%
  arrange(posted_date) %>%
  slice(1) %>%
  ungroup() %>%
  # Select only relevant fields with unique values
  select(source, doi, posted_date, title, abstract) %>%
  distinct()

```



# Create final dataset (DataCite only)

```{r}

sample_date <- "2020-04-12"

covid_preprints_datacite <- data_df_covid%>%
  filter(posted_date <= as.Date(sample_date))

covid_preprints_datacite %>%
  write_csv("data/covid19_preprints_datacite.csv")

```

# Visualizations

```{r}

# Default theme options
theme_set(theme_minimal() +
          theme(text = element_text(size = 12),
          axis.text.x = element_text(angle = 90, vjust = 0.5),
          axis.title.x = element_text(margin = margin(20, 0, 0, 0)),
          axis.title.y = element_text(margin = margin(0, 20, 0, 0)),
          legend.key.size = unit(0.5, "cm"),
          legend.text = element_text(size = 8),
          plot.caption = element_text(size = 8, color = "darkgrey", 
                                      margin = margin(20, 0, 0, 0))))

```


```{r}

# Repositories with < 10 preprints
other <- covid_preprints_datacite %>%
  count(source) %>%
  filter(n < 10) %>%
  pull(source)

# Daily preprint counts
covid_preprints_datacite %>%
  mutate(source = case_when(
    source %in% other ~ "Other*",
    T ~ source
  )) %>%
  count(source, posted_date) %>%
  ggplot(aes(x = posted_date, y = n, fill = source)) +
  geom_col() +
  labs(x = "Posted Date", y = "Preprints", fill = "Source",
       title = "COVID-19 preprints per day (DataCite)",
       subtitle = paste0("(up until ", sample_date, ")")) +
  scale_x_date(date_breaks = "7 days",
               date_minor_breaks = "1 day",
               expand = c(0.01, 0),
               limits = c(ymd("2020-01-15"), ymd(sample_date)+1)) +
  scale_fill_manual(values = usecol(pal_unikn_pair)) +
  ggsave("outputs/figures/covid19_preprints_datacite_day.png", width = 12, height = 6)

```


```{r}

# Weekly preprint counts
covid_preprints_datacite %>%
  mutate(
    source = case_when(
      source %in% other ~ "Other*",
      T ~ source
    ),
    posted_week = as.Date(cut(posted_date,
                                   breaks = "week",
                                   start.on.monday = TRUE))) %>%
  count(source, posted_week) %>%
  ggplot(aes(x = posted_week, y = n, fill = source)) +
  geom_col() +
  labs(x = "Posted Date (by week)", y = "Preprints", fill = "Source",
       title = "COVID-19 preprints per week (DataCite)", 
       subtitle = paste0("(up until ", sample_date, ")")) +
  scale_x_date(date_breaks = "1 week",
               expand = c(0.01, 0),
               limits = c(ymd("2020-01-13"), ymd(sample_date))) +
  scale_fill_manual(values = usecol(pal_unikn_pair)) +
  ggsave("outputs/figures/covid19_preprints_datacite_week.png", width = 12, height = 6)

```

```{r}

# Cumulative daily preprint counts
covid_preprints_datacite %>%
  mutate(source = case_when(
      source %in% other ~ "Other*",
      T ~ source
    )) %>%
  count(source, posted_date) %>%
  complete(posted_date, nesting(source), fill = list(n = 0)) %>%
  group_by(source) %>%
  arrange(posted_date) %>%
  mutate(cumulative_n = cumsum(n)) %>%
  ggplot() +
  geom_area(aes(x = posted_date, y = cumulative_n, fill = source)) +
  labs(x = "Posted Date", y = "Preprints", fill = "Source",
       title = "COVID-19 preprints (cumulative) (DataCite)", 
       subtitle = paste0("(up until ", sample_date, ")")) +
  scale_y_continuous(labels = scales::comma) +
  scale_x_date(date_breaks = "1 week",
               expand = c(0.01, 0),
               limits = c(ymd("2020-01-13"), ymd(sample_date))) +
  scale_fill_manual(values = usecol(pal_unikn_pair)) +
  theme_minimal() +
  theme(text = element_text(size = 12),
        axis.text.x = element_text(angle = 90, vjust = 0.5),
        axis.title.x = element_text(margin = margin(20, 0, 0, 0)),
        axis.title.y = element_text(margin = margin(0, 20, 0, 0)),
        legend.key.size = unit(0.5, "cm"),
        legend.text = element_text(size = 8),
        plot.caption = element_text(size = 8, color = "darkgrey", margin = margin(20, 0, 0, 0))) +
  ggsave("outputs/figures/covid19_preprints_datacite_day_cumulative.png", width = 12, height = 6)

```

